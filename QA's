# Question 1 (GAN Architecture)

1. **Generator (G):** Learns to generate realistic data (e.g., images) from random noise.
2. **Discriminator (D):** Learns to distinguish real data from fake data generated by G.

### Objectives

* **Generator’s Goal:** Fool the discriminator by generating data that is indistinguishable from real data.
* **Discriminator’s Goal:** Accurately classify inputs as real (from the dataset) or fake (from the generator).

This creates an **adversarial loop**:

* As G improves, it produces more realistic samples.
* As D improves, it becomes better at detecting fake samples.
* Both networks improve by optimizing their opposing objectives.

### Training Dynamics

* G and D are trained alternately.
* The discriminator receives real and generated samples and minimizes a **binary classification loss**.
* The generator tries to **maximize** the discriminator’s classification error on its fake outputs (i.e., make D think they are real).

### Diagram: GAN Architecture and Data Flow

```
          [Random Noise z]                     
                 │                                      
                 ▼                                     
          ┌───────────────┐                           
          │   Generator   │                           
          │      G(z)     │   Generates fake image     
          └──────┬────────┘                           
                 │                                     
     ┌───────────▼────────────┐                         
     │      Discriminator      │                         
     │    D(real or fake?)     │                         
     └─────────┬──────────────┘                         
               │                                      
      Real/Fake Decision (Loss)                      
```

* **Top path:** Noise vector `z` → Generator → Fake image → Discriminator → Output: Fake
* **Parallel path:** Real image from dataset → Discriminator → Output: Real

### Summary

The adversarial process results in the generator producing increasingly realistic data. Ideally, 
the process converges when the discriminator can no longer tell real from fake, i.e., 50% accuracy.









# Question 2 (Ethics and AI Harm)

### Selected Harm: **Representational Harm**
### Real-World Example:

A photo-enhancement AI system designed to “beautify” profile pictures may systematically lighten skin tones, narrow facial features, or apply Eurocentric beauty standards. This sends a harmful message that certain appearances are preferable, reinforcing stereotypes and marginalizing people of color or those with non-Western features.

### Mitigation Strategies:

1. **Diverse and Inclusive Training Data:**
   Ensure the dataset includes balanced representation across races, ethnicities, genders, and facial features. This reduces bias in the model’s output and teaches the AI to handle diversity fairly.

2. **Human-Centered Evaluation and Feedback:**
   Involve diverse stakeholders in reviewing model outputs for bias and harm. User feedback loops, especially from underrepresented communities, help continuously detect and correct representational issues.









# Question 5 (Legal and Ethical Implications of GenAI)

### Legal and Ethical Concerns of AI-Generated Content

#### 1. **Memorizing Private Data**

Generative models like GPT-2 have been found to regurgitate private or sensitive data from training datasets, such as names, email addresses, or chat logs. This poses:

* **Legal issues** under data protection laws like GDPR, which grants individuals the right to privacy and control over their personal data.
* **Ethical issues** around consent and harm, especially when individuals are unaware their data is being used or exposed in AI outputs.

#### 2. **Generating Copyrighted Material**

Models trained on copyrighted content, like entire books (e.g., *Harry Potter*), can sometimes reproduce large chunks of protected text. This raises:

* **Legal concerns** about violating copyright laws when models essentially act as unauthorized duplicators.
* **Ethical concerns** about fairness to content creators, who may lose control over how their work is used or profited from.

### Should Generative AI Be Restricted from Certain Data During Training?

**Yes — restrictions should apply.**

#### Justification:

1. **To Protect Individual Privacy:**
   Allowing models to train on unfiltered public data (e.g., forum posts, social media) risks embedding sensitive, personally identifiable information. Clear consent and data curation are essential.

2. **To Respect Copyright and Fair Use:**
   Creators deserve control over how their work is reused. Training on copyrighted content without permission undermines intellectual property rights and can result in legal disputes (as seen with lawsuits against AI companies).

### Balanced Approach:

Rather than banning data types outright, AI developers should:

* Use **curated and licensed datasets**.
* Apply **differential privacy** techniques during training.
* Provide **opt-out mechanisms** for creators and individuals.









# Question 6 (Bias & Fairness Tools)

### Bias Metric: **False Negative Rate Parity**

#### **What It Measures:**

False Negative Rate (FNR) parity compares how often different groups are incorrectly predicted to have a negative outcome when they should have been positive. In binary classification, it’s calculated as:

FNR = FN / (FN + TP)

FNR parity means that this rate is approximately equal across all demographic groups (e.g., race, gender).

#### **Why It’s Important:**

FNR parity ensures that no group is disproportionately denied access or opportunity due to missed positive predictions. For example, in a loan approval model, if FNR is much higher for Black applicants, it means they are unfairly being denied loans more often despite qualifying — a serious equity issue.

#### **How a Model Might Fail This Metric:**

A model could have high FNR for one group if:

* The training data is imbalanced or biased.
* Features correlate differently across groups but the model treats them uniformly.
* It optimizes overall accuracy without considering subgroup fairness.

This leads to **disparate impact**, especially in high-stakes domains like hiring, healthcare, or criminal justice.
